{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4074f1b-2ec7-48dc-aa7c-f153ecb505d3",
   "metadata": {},
   "source": [
    "# PINN for generating thermography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d55862d-3dd6-435f-bf8b-394589b23aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c1ec83-2f26-4173-ad6b-4c4fad68d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thermography_preprocessing import ThermoDataPreparation\n",
    "from utils import get_meshgrids3D_flattend, select_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479fdb4-ffc1-455b-be66-e41e8276232b",
   "metadata": {},
   "source": [
    "## PINN(a): simple heat equation (ignore the heat transfer caused by geometrical structre)\n",
    "\n",
    "### Simplified heat eq.\n",
    "$$\n",
    "  \\frac{\\partial T}{\\partial t} - \\alpha \\nabla^2 T = 0\n",
    "$$\n",
    "where **T** is temperature, **α** is thermal diffusivity, and **∇²T** is the Laplacian (spatial diffusion).\n",
    "\n",
    "### Ideas of PINN\n",
    "* Loss function: $L = L_{data} + L_{phy}$\n",
    "* Data loss $L_{data}$: boundary conditions, checked for a set of sample points (< all sample points)\n",
    "* Physics Loss $L_{phy}$: physic loss, checked for **ALL** sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0798ccfd-77ec-4e99-80c6-1c82cae910a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the physics-informed neural network (PINN)\n",
    "class HeatPINNSimple(nn.Module):\n",
    "    def __init__(self, layers=[3, 50, 50, 50, 1]):\n",
    "        super(HeatPINNSimple, self).__init__()\n",
    "        self.model = nn.Sequential(*[nn.Sequential(nn.Linear(layers[i], layers[i+1]), nn.Tanh()) for i in range(len(layers)-2)], nn.Linear(layers[-2], layers[-1]))\n",
    "        # Trainable parameters (unknowns in the equation)\n",
    "        self.alpha = nn.Parameter(torch.tensor(1.0))  # Thermal conductivity, according to wiki, it is apprently sufficient to set alpha=1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "\n",
    "class TrainingPINNSimple():\n",
    "    \"\"\"\n",
    "    Note\n",
    "    ----\n",
    "        According to Moseley_20, training sample points for selected anew for each epoch\n",
    "        \"For each update step a random set of \n",
    "            * discretised points are sampled from the initial wavefield to compute the boundary loss \n",
    "            * a random set of continuous points over the full input space ... to compute the physics loss.\"\n",
    "\n",
    "    Considerations (v250228)\n",
    "    --------------\n",
    "        * I need the info of the intervals, dx, dy and dt\n",
    "        * Otherwise, these intervals will be treated equally via fixed alpha\n",
    "        * idea; should I make alpha anisotropic?? i.e. alph = [alpha_y, alph_x, alph_t]?? What should I change in the heat eq for that? \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = HeatPINNSimple()\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, _lr):\n",
    "        self._optimizer = optim.Adam(self.model.parameters(), lr=_lr)\n",
    "\n",
    "    #==== (a) Physics loss\n",
    "    def physics_loss(self, Nx, Ny, Nt, size=0.3):\n",
    "        # (1) Model the thermo. values: modeling points need to be selected first\n",
    "        xyt_pde = torch.from_numpy(self.select_pde_points(Nx, Ny, Nt, size)) # = modeling positions\n",
    "        xyt_pde.requires_grad = True\n",
    "        T_pred = self.model(xyt_pde) #self.model.forward(xyt_pde)\n",
    "        # (2) Partial derivatives\n",
    "        grads = torch.autograd.grad(T_pred, xyt_pde, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
    "        T_t = grads[:, 2:3]  # dT/dt\n",
    "        T_xx = torch.autograd.grad(grads[:, 0:1], _data_xyt, grad_outputs=torch.ones_like(grads[:, 0:1]), create_graph=True)[0][:, 0:1]  # d²T/dx²\n",
    "        T_yy = torch.autograd.grad(grads[:, 1:2], _data_xyt, grad_outputs=torch.ones_like(grads[:, 1:2]), create_graph=True)[0][:, 1:2]  # d²T/dy²\n",
    "        # (3) Loss\n",
    "        residual = T_t - self.model.alpha * (T_xx + T_yy)\n",
    "        return torch.mean(residual**2)\n",
    "\n",
    "    def select_pde_points(self, Nx, Ny, Nt, size):\n",
    "        # Output = numpy array\n",
    "        rng = np.random.default_rng(seed=None)\n",
    "        n = int(size*Nx* Ny* Nt) # Number of sampling pints for computing the PDE\n",
    "        return np.array([rng.uniform(size=n)*Ny, rng.uniform(size=n)*Nx, rng.uniform(size=n)*Nt])\n",
    "\n",
    "    #==== (b) Data fidelity loss    \n",
    "    def data_fidelity_loss(self, _data_T):\n",
    "        # Randomly select the sampling positions (as indices)\n",
    "        idx_bp, bp = self.select_boundary_points(_data_T)\n",
    "        # Prediction\n",
    "        T_pred = self.model(torch.from_numpy(bp)) #self.model.forward(torch.from_numpy(bp))\n",
    "        return torch.mean((T_pred - torch.from_numpy(_data_T.flatten('F')[idx_bp])) ** 2)\n",
    "\n",
    "    def select_boundary_points(self, _data_T, size=0.1):\n",
    "        # Output = numpy array\n",
    "        Ny, Nx, Nt = _data_T.shape\n",
    "        xx, yy, tt = get_meshgrids3D_flattend(Nx, Ny, Nt) # each is in vector-form\n",
    "        idx_bp = select_indices(low=0, high=int(Nx*Ny*Nt), size=int(size*(Nx*Ny*Nt)), seed=None)\n",
    "        bp = np.array([yy[idx_bp], xx[idx_bp], tt[idx_bp]]).astype(float)\n",
    "        return (idx_bp, bp)\n",
    "        \n",
    "    #==== Training\n",
    "    def train(self, _data_T, n_epochs=50, epoch_phy=9, lambda_data=1.0, lambda_physics=1.0):\n",
    "        Ny, Nx, Nt = _data_T.shape\n",
    "        for epoch in range(n_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            # (a) Data fidelity loss\n",
    "            data_loss_value = self.data_fidelity_loss(_data_T)\n",
    "            # (b) Physics loss: activated after few epochs\n",
    "            if epoch > epoch_phy: \n",
    "                physics_loss_value = self.physics_loss(self, Nx, Ny, Nt)\n",
    "            else:\n",
    "                physics_loss_value = 0.0\n",
    "            total_loss =  lambda_data * data_loss_value + lambda_physics * physics_loss_value\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Total Loss: {total_loss.item():.6f}, Physics Loss: {physics_loss_value.item():.6f}, Data Loss: {data_loss_value.item():.6f}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "121a1d3b-3193-454f-97e0-40a78e3d3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) Load: Thermography data\n",
    "screwNo = 78 #78, 85, 88\n",
    "path_rel_1 = f'/Volumes/Sandisk_SD/Work/IZFP/ReMachine/Thermografie/4_InspectedSamples/B{screwNo}/measurements'\n",
    "pos_y, pos_x = (5, -2) # <- quite clean data (fileNo.59)\n",
    "prepper = ThermoDataPreparation()\n",
    "prepper.reader = path_rel_1\n",
    "data_T = prepper.get_processed_data_time(pos=(pos_y, pos_x), ymin=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d2ecb69-22b5-4d1f-b8c3-cdfb9304618d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m training \u001b[38;5;241m=\u001b[39m TrainingPINNSimple()\n\u001b[1;32m      3\u001b[0m training\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model_simple \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_T\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 81\u001b[0m, in \u001b[0;36mTrainingPINNSimple.train\u001b[0;34m(self, _data_T, n_epochs, epoch_phy, lambda_data, lambda_physics)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# (a) Data fidelity loss\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m data_loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_fidelity_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_data_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# (b) Physics loss: activated after few epochs\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m epoch_phy: \n",
      "Cell \u001b[0;32mIn[30], line 64\u001b[0m, in \u001b[0;36mTrainingPINNSimple.data_fidelity_loss\u001b[0;34m(self, _data_T)\u001b[0m\n\u001b[1;32m     62\u001b[0m idx_bp, bp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_boundary_points(_data_T)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m T_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#self.model.forward(torch.from_numpy(bp))\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean((T_pred \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(_data_T\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)[idx_bp])) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m, in \u001b[0;36mHeatPINNSimple.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/remachine_torch/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# # Initialize and train the model\n",
    "training = TrainingPINNSimple()\n",
    "training.optimizer = 0.001\n",
    "model_simple = training.train(data_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df76ce1-d205-4a53-b389-4cce6312ec3c",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "1. Data prep; select a set of \"clean\" data (max 5) -> segment it and save it in a folder (both for trainig and testing)\n",
    "2. Write a data pipline to use these data set  \n",
    "3. Train\n",
    "4. Test with a noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e35d54-12d0-499d-bc9e-34ad19297f3d",
   "metadata": {},
   "source": [
    "## PINN (b): Heat eq. with convection term (account for the heat transfer due to the geometrical structure)\n",
    "\n",
    "### Modified heat equation\n",
    "$$\\rho c_p \\frac{\\partial T}{\\partial t} = \\alpha \\nabla^2 T + Q - h A (T - T_{\\infty})$$\n",
    "with some (unknown) constants parameters, and some variables. \n",
    "\n",
    "#### Constant parameters\n",
    "1. **Density ($\\rho$)**, known (material parameter)\n",
    "2. **Specific Heat Capacity ($c_p$)**, known (material parameter)\n",
    "3. **Thermal Conductivity ($\\alpha$)** (for now, I will assume this to be constant), unknown\n",
    "4. **Convective Heat Transfer Coefficient ($h$)**, unknown\n",
    "5. **Ambient Temperature ($T_{\\infty}$)**, I don't know exactly, but between 20 to 25 Celsius, I guess\n",
    "6. **Internal Heat Generation ($Q$)**,  unknown\n",
    "\n",
    "#### Variables\n",
    "1. **Surface Area per Unit Volume ($A$)**, I want to treat it unknown\n",
    "2. **Temperature ($T$)**, unknown, values of interest to model with PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fe400-3cd6-475c-9354-45bd31ac5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the physics-informed neural network (PINN)\n",
    "class HeatPINN(nn.Module):\n",
    "    def __init__(self, layers=[3, 50, 50, 50, 1]):\n",
    "        super(HeatPINN, self).__init__()\n",
    "        self.model = nn.Sequential(*[nn.Sequential(nn.Linear(layers[i], layers[i+1]), nn.Tanh()) for i in range(len(layers)-2)], nn.Linear(layers[-2], layers[-1]))\n",
    "        \n",
    "        # Trainable parameters (unknowns in the equation)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.01))  # Thermal conductivity\n",
    "        self.h = nn.Parameter(torch.tensor(0.1))  # Convective heat transfer coefficient\n",
    "        self.Q = nn.Parameter(torch.tensor(0.0))  # Internal heat generation\n",
    "        self.A = nn.Parameter(torch.tensor(1.0))  # Surface area per unit volume\n",
    "        self.T_inf = nn.Parameter(torch.tensor(22.5))  # Ambient temperature (initial guess)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Generate collocation points (x, y, t) for PINN physics loss\n",
    "def generate_collocation_points(n_points=1000):\n",
    "    x = torch.rand(n_points, 1) * 10  # Assume screw thread cross-section (0,10)\n",
    "    y = torch.rand(n_points, 1) * 10\n",
    "    t = torch.rand(n_points, 1) * 5   # Time range (0,5 sec)\n",
    "    return torch.cat([x, y, t], dim=1)\n",
    "\n",
    "# Define the physics loss using the modified heat equation\n",
    "def physics_loss(model, collocation_points, rho=7850, c_p=500):\n",
    "    collocation_points.requires_grad = True\n",
    "    T_pred = model(collocation_points)\n",
    "    \n",
    "    grads = torch.autograd.grad(T_pred, collocation_points, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
    "    \n",
    "    T_t = grads[:, 2:3]  # dT/dt\n",
    "    T_xx = torch.autograd.grad(grads[:, 0:1], collocation_points, grad_outputs=torch.ones_like(grads[:, 0:1]), create_graph=True)[0][:, 0:1]  # d²T/dx²\n",
    "    T_yy = torch.autograd.grad(grads[:, 1:2], collocation_points, grad_outputs=torch.ones_like(grads[:, 1:2]), create_graph=True)[0][:, 1:2]  # d²T/dy²\n",
    "    \n",
    "    alpha = model.alpha\n",
    "    h = model.h\n",
    "    Q = model.Q\n",
    "    A = model.A\n",
    "    T_inf = model.T_inf\n",
    "    \n",
    "    residual = rho * c_p * T_t - alpha * (T_xx + T_yy) - Q + h * A * (T_pred - T_inf)\n",
    "    return torch.mean(residual**2)\n",
    "\n",
    "# Define the data fidelity loss\n",
    "def data_fidelity_loss(model, data_xyt, data_T):\n",
    "    T_pred = model(data_xyt)\n",
    "    return torch.mean((T_pred - data_T) ** 2)\n",
    "\n",
    "# Training loop\n",
    "def train_pinn(model, optimizer, data_xyt, data_T, n_epochs=1000, lambda_data=1.0, lambda_physics=1.0):\n",
    "    collocation_points = generate_collocation_points()\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        physics_loss_value = physics_loss(model, collocation_points)\n",
    "        data_loss_value = data_fidelity_loss(model, data_xyt, data_T)\n",
    "        total_loss = lambda_physics * physics_loss_value + lambda_data * data_loss_value\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Total Loss: {total_loss.item():.6f}, Physics Loss: {physics_loss_value.item():.6f}, Data Loss: {data_loss_value.item():.6f}\")\n",
    "\n",
    "# Example dataset (replace with real data)\n",
    "data_xyt = torch.rand(100, 3) * torch.tensor([10, 10, 5])  # Example (x,y,t) points\n",
    "data_T = torch.rand(100, 1) * 100  # Example temperature values\n",
    "\n",
    "# Initialize and train the model\n",
    "pinn_model = HeatPINN()\n",
    "optimizer = optim.Adam(pinn_model.parameters(), lr=0.001)\n",
    "train_pinn(pinn_model, optimizer, data_xyt, data_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c000b-9105-4e73-af21-0c4086863f5c",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "* should I go for the freq. domain directly because we are interested in one frequency anyway? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce1609-1d21-4e1e-9510-4409b7457052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
